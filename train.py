import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import os

# æ–‡ä»¶ç”¨é€”ï¼šè®­ç»ƒå…¥å£ï¼Œç”Ÿæˆæ‰‹åŠ¿è¯†åˆ«æ¨¡å‹æƒé‡
# æœ€åä¿®æ”¹ï¼š2025-12-04
# ä¸»è¦åŠŸèƒ½ï¼š
# - è¯»å– data/*.npy å¹¶åšæ ‡ç­¾ç¼–ç 
# - è®­ç»ƒ MLP å¹¶è¯„ä¼°åˆ†ç±»ç»“æœ
# - ä¿å­˜æƒé‡ä¸ç±»åˆ«é›†åˆåˆ° rps_mlp.pth
# é‡è¦ç»„ä»¶ï¼šMLPã€DataLoaderã€CrossEntropyLossã€Adam
# ä½¿ç”¨è¯´æ˜ï¼šé‡‡é›†æ•°æ®åè¿è¡Œæœ¬æ–‡ä»¶ä»¥æ›´æ–°æ¨¡å‹æƒé‡ã€‚
# ---------- é…ç½® ----------
DATA_DIR = "data"
MODEL_PATH = "rps_mlp.pth"
EPOCHS = 80
BATCH_SIZE = 32
LR = 1e-3
HIDDEN = 128
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# --------------------------

# 1. è¯»å–æ•°æ®
X = np.load(os.path.join(DATA_DIR, "dataset.npy"))
y = np.load(os.path.join(DATA_DIR, "labels.npy"))
print("âœ… æ•°æ®åŠ è½½å®Œæˆ:", X.shape, y.shape)
# è§£é‡Šï¼šX å½¢å¦‚ [æ ·æœ¬æ•°, 63]ï¼›y æ˜¯åŒç­‰é•¿åº¦çš„æ ‡ç­¾æ•°ç»„ï¼ˆå­—ç¬¦ä¸²ï¼‰ã€‚

# 2. æ ‡ç­¾ç¼–ç 
le = LabelEncoder()
y_enc = le.fit_transform(y)
num_classes = len(le.classes_)
print("ç±»åˆ«:", le.classes_)
# è§£é‡Šï¼šæŠŠå­—ç¬¦ä¸²æ ‡ç­¾è½¬æˆæ•°å­— 0..(num_classes-1)ï¼Œå¹¶ä¿å­˜ç±»åˆ«åä»¥ä¾¿æ¨ç†æ—¶æ˜¾ç¤ºã€‚

# 3. è½¬ Tensor
X_train, X_test, y_train, y_test = train_test_split(
    X, y_enc, test_size=0.2, random_state=42, stratify=y_enc
)
# è§£é‡Šï¼š20% æµ‹è¯•é›†ï¼Œå›ºå®šéšæœºç§å­ 42 ä¿è¯ç»“æœå¯å¤ç°ï¼›stratify ä¿è¯å„ç±»æ¯”ä¾‹åœ¨è®­ç»ƒ/æµ‹è¯•ä¸­å¤§è‡´ç›¸åŒã€‚
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)
# è§£é‡Šï¼š
# - float32ï¼šç‰¹å¾ç”¨æµ®ç‚¹æ•°ã€‚
# - longï¼šæ ‡ç­¾ç”¨æ•´å‹ï¼ˆåˆ†ç±»æ—¶éœ€è¦ï¼‰ã€‚

train_loader = torch.utils.data.DataLoader(
    torch.utils.data.TensorDataset(X_train, y_train),
    batch_size=BATCH_SIZE,
    shuffle=True,
)
test_loader = torch.utils.data.DataLoader(
    torch.utils.data.TensorDataset(X_test, y_test),
    batch_size=BATCH_SIZE,
)
# è§£é‡Šï¼šDataLoader ä¼šæŠŠæ•°æ®æŒ‰ batch åˆ†ç»„ï¼Œè®­ç»ƒæ—¶ shuffle=True æ‰“ä¹±é¡ºåºæ›´åˆ©äºå­¦ä¹ ã€‚

# 4. å®šä¹‰ MLP æ¨¡å‹
class MLP(nn.Module):
    """ä¸‰å±‚æ„ŸçŸ¥æœºï¼Œç”¨äºæ‰‹åŠ¿åˆ†ç±»"""
    def __init__(self, in_dim, hidden, num_classes):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, num_classes),
        )

    def forward(self, x):
        return self.net(x)

model = MLP(X.shape[1], HIDDEN, num_classes).to(DEVICE)
optimizer = optim.Adam(model.parameters(), lr=LR)
criterion = nn.CrossEntropyLoss()
# è§£é‡Šï¼š
# - to(DEVICE)ï¼šå¦‚æœæ”¯æŒ CUDA å°±æ”¾åˆ°æ˜¾å¡ä¸Šè®­ç»ƒï¼›å¦åˆ™ç”¨ CPUã€‚
# - Adamï¼šä¼˜åŒ–å™¨ï¼›CrossEntropyLossï¼šå¤šåˆ†ç±»å¸¸ç”¨æŸå¤±å‡½æ•°ã€‚

# 5. è®­ç»ƒ
for epoch in range(1, EPOCHS + 1):
    model.train()
    total_loss = 0
    for xb, yb in train_loader:
        xb, yb = xb.to(DEVICE), yb.to(DEVICE)
        out = model(xb)
        loss = criterion(out, yb)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * xb.size(0)
    avg_loss = total_loss / len(train_loader.dataset)
    if epoch % 10 == 0 or epoch == 1:
        print(f"Epoch {epoch:03d}/{EPOCHS} | loss={avg_loss:.4f}")

# 6. è¯„ä¼°
model.eval()
preds, targets = [], []
with torch.no_grad():
    for xb, yb in test_loader:
        out = model(xb.to(DEVICE))
        preds.extend(out.argmax(1).cpu().numpy())
        targets.extend(yb.numpy())

print("âœ… åˆ†ç±»ç»“æœï¼š")
print(classification_report(targets, preds, target_names=le.classes_))

# 7. ä¿å­˜æ¨¡å‹å’Œæ ‡ç­¾ç¼–ç 
torch.save({
    "model_state": model.state_dict(),
    "classes": le.classes_,
}, MODEL_PATH)
print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {MODEL_PATH}")
# è§£é‡Šï¼šä¿å­˜æ¨¡å‹æƒé‡(state_dict)å’Œç±»åˆ«åï¼Œæ–¹ä¾¿æ¨ç†è„šæœ¬ç›´æ¥åŠ è½½ä½¿ç”¨ã€‚
